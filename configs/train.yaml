defaults:
  - _self_

hydra:
  output_subdir: null

model:
  name: "ordinal_progressive_sd"
  embedding_dim: 768
  conditioning_dim: 768
  base_channels: 320
  num_res_blocks: [2, 2, 2, 2]
  attention_heads: 8
  dropout: 0.1
  latent_channels: 4
  use_pretrained_vae: true
  pretrained_vae_path: "CompVis/stable-diffusion-v1-4"
  pretrained_unet_path: "CompVis/stable-diffusion-v1-4"

  ordinal_embedder:
    type: "aoe"
    num_classes: 4
    interpolation_steps: 101
    boe:
      weight: 1.0
    aoe:
      delta_scale: 0.05  # Reduced from 0.15 for more stable embeddings

optimizer:
  name: "adamw"
  lr: 1e-5
  weight_decay: 0.001
  betas: [0.9, 0.999]

scheduler:
  name: "cosine"
  warmup_epochs: 2
  max_epochs: 75
  min_lr: 1e-6

dataset:
  dataset_path: /home/umut_dundar/repositories/progressive-stable-diffusion/data/processed_data
  batch_size: 32
  num_workers: 14
  persistent_workers: true     # Keep workers alive between epochs (faster)
  prefetch_factor: 2           # Number of batches to prefetch per worker
  image_size: 256
  num_classes: 4
  sampler: "class_balanced"
  augmentation:
    flip: true
    rotation: 5
    center_crop: 224
    color_jitter: True

training:
  max_epochs: 100
  log_every_n_steps: 50
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  precision: "16-mixed"        # Use 16-mixed for better AMP support
  accelerator: "gpu"
  devices: 1
  strategy: "auto"
  seed: 42
  ema_decay: 0.999
  update_starting_at_step: 100
  update_every_n_steps: 4
  use_min_snr_weighting: true
  use_compile: False           # torch.compile (PyTorch 2.0+) - set true for ~20% speedup
  compile_mode: "reduce-overhead"  # Options: "default", "reduce-overhead", "max-autotune"
  gradient_checkpointing: false    # Trade compute for memory - enables larger batch sizes
  noise_offset: 0.0            # Improves contrast in generated images (0.0 to disable)
  input_perturbation: 0.1      # Perturb input noise for better training (0.0-0.1)


wandb:
  project: "prog-disease-generation"
  run_name: "ordinal-aware-diffusion"
  log_samples_every_n_steps: 50
  offline: True

diffusion:
  noise_schedule: "linear"
  beta_start: 0.00085
  beta_end: 0.012
  num_train_timesteps: 1000
  sampling_steps: 50
  guidance_scale: 2.0
  min_snr_gamma: 1.0
  ema_update_interval: 1
  nodal_scale: 1.0
